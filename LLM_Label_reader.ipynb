{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZiG6fYg-xbe",
        "outputId": "11b1a4c7-5ae5-4b59-badb-d498db994d32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 5.27.1 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.4 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.1 which is incompatible.\n",
            "google-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 5.27.1 which is incompatible.\n",
            "google-cloud-aiplatform 1.54.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.1 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.1 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.25.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.1 which is incompatible.\n",
            "google-cloud-datastore 2.15.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.1 which is incompatible.\n",
            "google-cloud-firestore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.1 which is incompatible.\n",
            "google-cloud-functions 1.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.1 which is incompatible.\n",
            "google-cloud-iam 2.15.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.1 which is incompatible.\n",
            "google-cloud-language 2.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.1 which is incompatible.\n",
            "google-cloud-resource-manager 1.12.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.1 which is incompatible.\n",
            "google-cloud-translate 3.11.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.1 which is incompatible.\n",
            "grpc-google-iam-v1 0.13.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.1 which is incompatible.\n",
            "proto-plus 1.23.0 requires protobuf<5.0.0dev,>=3.19.0, but you have protobuf 5.27.1 which is incompatible.\n",
            "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.27.1 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 5.27.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.41.2 requires huggingface-hub<1.0,>=0.23.0, but you have huggingface-hub 0.20.3 which is incompatible.\n",
            "transformers 4.41.2 requires tokenizers<0.20,>=0.19, but you have tokenizers 0.15.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip -qqq install pip --progress-bar off\n",
        "!pip -qqq install langchain-groq==0.1.3 --progress-bar off\n",
        "!pip -qqq install langchain==0.1.17 --progress-bar off\n",
        "!pip -qqq install llama-parse==0.1.3 --progress-bar off\n",
        "!pip -qqq install qdrant-client==1.9.1  --progress-bar off\n",
        "!pip -qqq install \"unstructured[md]\"==0.13.6 --progress-bar off\n",
        "!pip -qqq install fastembed==0.2.7 --progress-bar off\n",
        "!pip -qqq install flashrank==0.2.4 --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import textwrap\n",
        "from pathlib import Path\n",
        "\n",
        "from google.colab import userdata\n",
        "from IPython.display import Markdown\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import FlashrankRerank\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Qdrant\n",
        "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
        "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "from llama_parse import LlamaParse\n",
        "\n",
        "\n",
        "# set up gorp api key\n",
        "os.environ[\"GROQ_API_KEY\"] = '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'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def print_response(response):\n",
        "    response_txt = response[\"result\"]\n",
        "    for chunk in response_txt.split(\"\\n\"):\n",
        "        if not chunk:\n",
        "            print()\n",
        "            continue\n",
        "        print(\"\\n\".join(textwrap.wrap(chunk, 100, break_long_words=False)))\n"
      ],
      "metadata": {
        "id": "aGcQoSHT_caJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import Tool"
      ],
      "metadata": {
        "id": "-0PYSKjvfahm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#upload the pdfs in data"
      ],
      "metadata": {
        "id": "-E1hXwG-_yuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "\n",
        "# !gdown 1ee-BhQiH-S9a2IkHiFbJz9eX_SfcZ5m9 -O \"data/meta-earnings.pdf\""
      ],
      "metadata": {
        "id": "f3rPsfQa_jsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "from langchain.tools import BaseTool"
      ],
      "metadata": {
        "id": "iNwDFhASkIyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create OCR Tool"
      ],
      "metadata": {
        "id": "BZiheVSHigB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OCRTool():\n",
        "    def __init__(self,img_path):\n",
        "      self.img_path = img_path\n",
        "\n",
        "\n",
        "    def run(self):\n",
        "      # Prepare the payload\n",
        "      # API endpoint and headers\n",
        "      url = \"https://pen-to-print-handwriting-ocr.p.rapidapi.com/recognize/\"\n",
        "      headers = {\n",
        "    \"X-RapidAPI-Key\": \"1ff515ad07mshab205ba81f47020p1c9e1bjsnde5813fcd36f\",\n",
        "    \"X-RapidAPI-Host\": \"pen-to-print-handwriting-ocr.p.rapidapi.com\"}\n",
        "\n",
        "      # Read the image file as bytes\n",
        "      with open(self.img_path, 'rb') as f:\n",
        "          image_bytes = f.read()\n",
        "\n",
        "      payload = {\n",
        "          \"Session\": \"string\"\n",
        "      }\n",
        "\n",
        "      # Create the files dictionary with the image file\n",
        "      files = {\n",
        "          \"srcImg\": (self.img_path, image_bytes)\n",
        "      }\n",
        "\n",
        "      # Send the POST request\n",
        "      response = requests.post(url, data=payload, files=files, headers=headers)\n",
        "\n",
        "      # Check the response\n",
        "      if response.status_code == 200:\n",
        "          return (response.json().get('value'))\n",
        "      else:\n",
        "          print(\"OCR Error:\", response.status_code)\n",
        "          print(response.text)"
      ],
      "metadata": {
        "id": "6VADNsQ0gJk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model loading"
      ],
      "metadata": {
        "id": "gfugy-HVAG7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\")"
      ],
      "metadata": {
        "id": "JpnxOxfUAGEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#example one"
      ],
      "metadata": {
        "id": "WvUcV3udAktp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent\n"
      ],
      "metadata": {
        "id": "Zyt48duKpz3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "HXcLuz6VAVFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ocr = OCRTool(img_path = '/content/product.png')\n",
        "label = ocr.run()"
      ],
      "metadata": {
        "id": "BcMNd6eEp077"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a nutrition specialist. You will be given label information from the back of a packaged product. You are supposed to describe how healthy or unhealthy the product might be based on the Indian standards of healthy ingridient levels. Answer in pointers. In each pointer, along with the consequences also mention the indian healthy standards. And give a final conclusion at the end.\"),\n",
        "    (\"user\", f\"{label}\")\n",
        "])"
      ],
      "metadata": {
        "id": "wsHXK2zcALI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm | output_parser"
      ],
      "metadata": {
        "id": "7dGNIgMcAV31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "xzCSztaDxOCt",
        "outputId": "f9fb8b30-05c8-4ce7-caf6-1abe5f02b345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'zu re thore chips claim is in comparison\\nwith previous pack of Doritos (Rs. 25/39g),\\ncalculated on net quantity per rupee.\\nSource Euromonitor International Limited;\\npackaged Food; Nacho chip as per Tortilla\\nhp dekgition, in retail value RSP terms, all\\nretail channels, previous year data.\\nDoritos is he Registered Trademark\\nof PepsiCo, Inc.\\nPROPRIETARY FOOD - NAMKEEN (15.1)\\nINGREDIENTS: Corn (77%), Edible Vegetable Oi,\\n(Palmolein), *Seasoning (Milk Solids, Refined\\nWheat Flour (Maida), lodised Salt, Flavour\\n(Natural and Nature Identical Flavouring\\nSubstances), Cheese Powder, Tomato Powder,\\n\"Spices & Condiments, Anticaking Agent (551),\\nColor (160c), Acidity Regulator (330)).\\n*As flavouring agent. ~ Contains Onion and Garlic.\\nALLERGEN ADVICE: Contains Milk, Wheat.\\nNUTRITIONAL INFORMATION^\\nServe Size - 20 g^^\\nNutrients\\nPer 100 g\\nEnergy\\n499 kcal\\n5%\\nProtein\\n7.1 g\\nCarbohydrate\\n65.1 g\\nTotal Sugars\\n4.5 g\\nAdded Sugars\\n0.0 g\\n0%\\nTotal Fat\\n23.4 g\\n7%\\nSaturated Fat\\n9.0 g\\n8%\\nTrans Fat\\n0.1 g\\n1%\\nSodium\\n605 mg\\n6%\\n^Approximate\\n%RDA\\nPer Serve\\n^^Pack <20 g is a single consumption pack\\nANI BAL OF ALL TAVEOL'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.invoke({\"input\" : f\"Ans\"}))"
      ],
      "metadata": {
        "id": "lXWrMxC8Am4V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5c349f6-1b18-4db5-bfb4-9ad6de86c8cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's the analysis of the nutritional information of the Doritos Nacho Chips:\n",
            "\n",
            "* **High Energy Content**: The product has 499 kcal per 100g, which is high in energy density. This can contribute to weight gain and obesity if consumed excessively. Indian healthy standards recommend limiting energy intake from snacks to 100-150 kcal per serving.\n",
            "\n",
            "* **High in Carbohydrates**: The product contains 65.1g of carbohydrates per 100g, which is high. While some carbohydrates are necessary, excessive consumption can lead to insulin resistance and weight gain. Indian healthy standards recommend limiting carbohydrate intake from snacks to 20-30g per serving.\n",
            "\n",
            "* **High in Total Fat**: The product contains 23.4g of total fat per 100g, which is high. High fat intake can contribute to weight gain, obesity, and increased risk of cardiovascular disease. Indian healthy standards recommend limiting total fat intake from snacks to 10-15g per serving.\n",
            "\n",
            "* **High in Saturated Fat**: The product contains 9.0g of saturated fat per 100g, which is high. High saturated fat intake can increase the risk of cardiovascular disease. Indian healthy standards recommend limiting saturated fat intake from snacks to 3-5g per serving.\n",
            "\n",
            "* **High in Sodium**: The product contains 605mg of sodium per 100g, which is high. High sodium intake can increase blood pressure and contribute to cardiovascular disease. Indian healthy standards recommend limiting sodium intake from snacks to 200-300mg per serving.\n",
            "\n",
            "* **Presence of Added Preservatives and Colors**: The product contains anticaking agent (551), color (160c), and acidity regulator (330), which can be detrimental to health if consumed excessively.\n",
            "\n",
            "* **Contains Milk and Wheat**: The product contains milk and wheat, making it unsuitable for individuals with lactose intolerance or gluten intolerance.\n",
            "\n",
            "**Conclusion**: Based on the nutritional information, Doritos Nacho Chips can be considered an unhealthy snack option due to its high energy, carbohydrate, total fat, saturated fat, and sodium content. Additionally, the presence of added preservatives and colors can be detrimental to health. It is recommended to consume this product in moderation and balance it with a healthy and balanced diet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vector store and DB"
      ],
      "metadata": {
        "id": "Sm3Wht-ZAv-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")\n",
        "\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "qKtHRCCaAYag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "loader = DirectoryLoader(\"../\", glob=\"**/*.pdf\", use_multithreading=True)\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "jA2eAQJvBDH_",
        "outputId": "aa0ac5a2-a35b-4a29-f741-f05802ff5c1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain_community'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6e1bd115dd90>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_loaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDirectoryLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDirectoryLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"**/*.pdf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_multithreading\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_community'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "j-aQ9OzfAfbE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "021aaef2-ad9c-485e-c1ab-c71b15cb5f80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = FastEmbedEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")"
      ],
      "metadata": {
        "id": "AzcHu8mSCdq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter()\n",
        "documents = text_splitter.split_documents(docs)\n",
        "vector = FAISS.from_documents(documents, embeddings)"
      ],
      "metadata": {
        "id": "oWl20CTuAiE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {input}\"\"\")\n",
        "\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)"
      ],
      "metadata": {
        "id": "eQf87kitCPA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "retriever = vector.as_retriever()\n",
        "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
      ],
      "metadata": {
        "id": "1DrLQs6dCs6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
        "print(response[\"answer\"])\n",
        "\n",
        "# LangSmith offers several features that can help with testing:..."
      ],
      "metadata": {
        "id": "QReO_94lC81c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#chat history aware"
      ],
      "metadata": {
        "id": "X-JG2yEjC_eT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "])\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "\n",
        "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
      ],
      "metadata": {
        "id": "2xkFlXkTDCGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
        "retrieval_chain.invoke({\n",
        "    \"chat_history\": chat_history,\n",
        "    \"input\":  \"her name is 3,  tell  me how langsmith is helpful\"\n",
        "})"
      ],
      "metadata": {
        "id": "bfuKAsohDH9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A5KnGLHhDJR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Langserve api"
      ],
      "metadata": {
        "id": "-l-UVZxGDMpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"langserve[all]\""
      ],
      "metadata": {
        "id": "TrvMev7KDO42"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}